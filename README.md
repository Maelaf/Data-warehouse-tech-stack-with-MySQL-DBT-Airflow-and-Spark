# Data-warehouse-tech-stack-with-MySQL-DBT-Airflow-and-Spark
The Goal is to build a data pipeline that recieves streams of data from sensors and analyse the data to provide key insights.


In Data (ucdavis.edu) you can find example parquet data, and or sensor data in CSV formats with size ~1.5Gb uncompressed each. 

 <h1>Tasks</h1>
 
 * create a data warehouse
 * Use an ETL tool (dbt)
 * An Orchestration Service(Airflow)
 * and use redash for reporting
